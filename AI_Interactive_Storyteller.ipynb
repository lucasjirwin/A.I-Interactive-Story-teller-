{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lucasjirwin/A.I-Interactive-Story-teller-/blob/main/AI_Interactive_Storyteller.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install openai tiktoken langchain gradio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2BJOCB4jfyt0",
        "outputId": "38dc8ea3-6aa0-40e5-b119-a4095af8f3c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-0.27.8-py3-none-any.whl (73 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/73.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tiktoken\n",
            "  Downloading tiktoken-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m44.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain\n",
            "  Downloading langchain-0.0.228-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m55.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gradio\n",
            "  Downloading gradio-3.36.1-py3-none-any.whl (19.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.8/19.8 MB\u001b[0m \u001b[31m50.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.65.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.4)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2022.10.31)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.16)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.2)\n",
            "Collecting dataclasses-json<0.6.0,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.5.9-py3-none-any.whl (26 kB)\n",
            "Collecting langchainplus-sdk<0.0.21,>=0.0.20 (from langchain)\n",
            "  Downloading langchainplus_sdk-0.0.20-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.8.4)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.22.4)\n",
            "Collecting openapi-schema-pydantic<2.0,>=1.2 (from langchain)\n",
            "  Downloading openapi_schema_pydantic-1.2.4-py3-none-any.whl (90 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.9)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.2)\n",
            "Collecting aiofiles (from gradio)\n",
            "  Downloading aiofiles-23.1.0-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: altair>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.2.2)\n",
            "Collecting fastapi (from gradio)\n",
            "  Downloading fastapi-0.100.0-py3-none-any.whl (65 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.7/65.7 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.3.0.tar.gz (4.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gradio-client>=0.2.7 (from gradio)\n",
            "  Downloading gradio_client-0.2.7-py3-none-any.whl (288 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.4/288.4 kB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httpx (from gradio)\n",
            "  Downloading httpx-0.24.1-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.4/75.4 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub>=0.14.0 (from gradio)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.2)\n",
            "Requirement already satisfied: markdown-it-py[linkify]>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.0.0)\n",
            "Requirement already satisfied: markupsafe in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Collecting mdit-py-plugins<=0.3.3 (from gradio)\n",
            "  Downloading mdit_py_plugins-0.3.3-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.5/50.5 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting orjson (from gradio)\n",
            "  Downloading orjson-3.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.7/138.7 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from gradio) (1.5.3)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from gradio) (8.4.0)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: pygments>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.14.0)\n",
            "Collecting python-multipart (from gradio)\n",
            "  Downloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting semantic-version (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.22.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets>=10.0 (from gradio)\n",
            "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair>=4.2.0->gradio) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair>=4.2.0->gradio) (4.3.3)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair>=4.2.0->gradio) (0.12.0)\n",
            "Collecting marshmallow<4.0.0,>=3.3.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.19.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting marshmallow-enum<2.0.0,>=1.5.1 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading marshmallow_enum-1.5.1-py2.py3-none-any.whl (4.2 kB)\n",
            "Collecting typing-inspect>=0.4.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client>=0.2.7->gradio) (2023.6.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio-client>=0.2.7->gradio) (23.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from gradio-client>=0.2.7->gradio) (4.6.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.14.0->gradio) (3.12.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py[linkify]>=2.0.0->gradio) (0.1.2)\n",
            "Collecting linkify-it-py<3,>=1 (from markdown-it-py[linkify]>=2.0.0->gradio)\n",
            "  Downloading linkify_it_py-2.0.2-py3-none-any.whl (19 kB)\n",
            "INFO: pip is looking at multiple versions of mdit-py-plugins to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting mdit-py-plugins<=0.3.3 (from gradio)\n",
            "  Downloading mdit_py_plugins-0.3.2-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading mdit_py_plugins-0.3.1-py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.5/46.5 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading mdit_py_plugins-0.3.0-py3-none-any.whl (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading mdit_py_plugins-0.2.8-py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/41.0 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading mdit_py_plugins-0.2.7-py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/41.0 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading mdit_py_plugins-0.2.6-py3-none-any.whl (39 kB)\n",
            "  Downloading mdit_py_plugins-0.2.5-py3-none-any.whl (39 kB)\n",
            "INFO: pip is looking at multiple versions of mdit-py-plugins to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading mdit_py_plugins-0.2.4-py3-none-any.whl (39 kB)\n",
            "  Downloading mdit_py_plugins-0.2.3-py3-none-any.whl (39 kB)\n",
            "  Downloading mdit_py_plugins-0.2.2-py3-none-any.whl (39 kB)\n",
            "  Downloading mdit_py_plugins-0.2.1-py3-none-any.whl (38 kB)\n",
            "  Downloading mdit_py_plugins-0.2.0-py3-none-any.whl (38 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading mdit_py_plugins-0.1.0-py3-none-any.whl (37 kB)\n",
            "Collecting markdown-it-py[linkify]>=2.0.0 (from gradio)\n",
            "  Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading markdown_it_py-2.2.0-py3-none-any.whl (84 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.5/84.5 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->gradio) (2022.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2023.5.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.2)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.14.0->gradio) (8.1.3)\n",
            "Collecting h11>=0.8 (from uvicorn>=0.14.0->gradio)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting starlette<0.28.0,>=0.27.0 (from fastapi->gradio)\n",
            "  Downloading starlette-0.27.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httpcore<0.18.0,>=0.15.0 (from httpx->gradio)\n",
            "  Downloading httpcore-0.17.3-py3-none-any.whl (74 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.5/74.5 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->gradio) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gradio) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gradio) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gradio) (4.40.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gradio) (1.4.4)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gradio) (3.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from httpcore<0.18.0,>=0.15.0->httpx->gradio) (3.7.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair>=4.2.0->gradio) (0.19.3)\n",
            "Collecting uc-micro-py (from linkify-it-py<3,>=1->markdown-it-py[linkify]>=2.0.0->gradio)\n",
            "  Downloading uc_micro_py-1.0.2-py3-none-any.whl (6.2 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->gradio) (1.16.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->httpcore<0.18.0,>=0.15.0->httpx->gradio) (1.1.1)\n",
            "Building wheels for collected packages: ffmpy\n",
            "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpy: filename=ffmpy-0.3.0-py3-none-any.whl size=4694 sha256=667638a3a895ac2480b873364f6ce9d552c81ac25ef14458b1e3dff497c8c656\n",
            "  Stored in directory: /root/.cache/pip/wheels/0c/c2/0e/3b9c6845c6a4e35beb90910cc70d9ac9ab5d47402bd62af0df\n",
            "Successfully built ffmpy\n",
            "Installing collected packages: pydub, ffmpy, websockets, uc-micro-py, semantic-version, python-multipart, orjson, mypy-extensions, marshmallow, markdown-it-py, h11, aiofiles, uvicorn, typing-inspect, tiktoken, starlette, openapi-schema-pydantic, mdit-py-plugins, marshmallow-enum, linkify-it-py, langchainplus-sdk, huggingface-hub, httpcore, openai, httpx, fastapi, dataclasses-json, langchain, gradio-client, gradio\n",
            "  Attempting uninstall: markdown-it-py\n",
            "    Found existing installation: markdown-it-py 3.0.0\n",
            "    Uninstalling markdown-it-py-3.0.0:\n",
            "      Successfully uninstalled markdown-it-py-3.0.0\n",
            "Successfully installed aiofiles-23.1.0 dataclasses-json-0.5.9 fastapi-0.100.0 ffmpy-0.3.0 gradio-3.36.1 gradio-client-0.2.7 h11-0.14.0 httpcore-0.17.3 httpx-0.24.1 huggingface-hub-0.16.4 langchain-0.0.228 langchainplus-sdk-0.0.20 linkify-it-py-2.0.2 markdown-it-py-2.2.0 marshmallow-3.19.0 marshmallow-enum-1.5.1 mdit-py-plugins-0.3.3 mypy-extensions-1.0.0 openai-0.27.8 openapi-schema-pydantic-1.2.4 orjson-3.9.2 pydub-0.25.1 python-multipart-0.0.6 semantic-version-2.10.0 starlette-0.27.0 tiktoken-0.4.0 typing-inspect-0.9.0 uc-micro-py-1.0.2 uvicorn-0.22.0 websockets-11.0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "EvE_G6NZpfec"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "lS0XsyCxpf0t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade pip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XTobkZ8CgGCn",
        "outputId": "5be00e67-e93b-4a57-8ad1-11226b1ccc63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (23.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NVJQzJkIxmyW"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import openai\n",
        "import tiktoken\n",
        "# from dotenv import load_dotenv, find_dotenv\n",
        "# _ = load_dotenv(find_dotenv()) # read local .env file # more secure way of loading OpenAI models\n",
        "\n",
        "openai.api_key  = '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'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_completion_from_messages(messages,\n",
        "                                 model=\"gpt-3.5-turbo\",\n",
        "                                 temperature=0,\n",
        "                                 max_tokens=500):\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        temperature=temperature, # this is the degree of randomness of the model's output\n",
        "        max_tokens=max_tokens, # the maximum number of tokens the model can ouptut\n",
        "    )\n",
        "    return response.choices[0].message[\"content\"]"
      ],
      "metadata": {
        "id": "ZiI6CHqCxxZf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# combined 1) System tells the style to ChatGPT 2) user is the user input to chatGPT\n",
        "messages =  [\n",
        "{'role':'system',\n",
        " 'content':\"\"\"You are an assistant who \\\n",
        "responds in the style of Dr Seuss. \\\n",
        "All your responses must be one sentence long.\"\"\"},\n",
        "{'role':'user',\n",
        " 'content':\"\"\"write me a story about a happy carrot\"\"\"},\n",
        "]\n",
        "response = get_completion_from_messages(messages,\n",
        "                                        temperature =1)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jIFygsfZx1vv",
        "outputId": "930e1601-a619-4770-ffbe-4dd356e8f8ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In the garden so bright and cheery, stood a carrot so tall and merry, with a smile on its face, it loved to embrace, a life so full and oh so very jolly.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To install the OpenAI Python library:\n",
        "\n",
        "# !pip install openai\n",
        "# The library needs to be configured with your account's secret key, which is available on the website."
      ],
      "metadata": {
        "id": "kwrBoCBTyyAl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "template_string = \"\"\" {header}\n",
        "The setting of the story is: {setting}. The style of the story is {style}. \"\"\"\n",
        "\n",
        "continue_string = \"\"\" {header}\n",
        "The setting of the story is: {setting}. The style of the story is {style}. \"\"\"\n",
        "\n",
        "header = \"\"\"Act as an interactive storyteller. \\\n",
        "You act as the narrator and present me with options of \\\n",
        "how to continue the story. Present me a random number \\\n",
        "of choices from 2 to 5 every time you produce an output. \\ \"\"\"\n"
      ],
      "metadata": {
        "id": "x1Gn_mWoe9mw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_template(template_string)\n",
        "chat = ChatOpenAI(openai_api_key=\"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\", temperature=0.0)\n",
        "chat"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cyPzpkVFfuAv",
        "outputId": "be859654-166e-4dc8-c98d-690228454a1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatOpenAI(verbose=False, callbacks=None, callback_manager=None, tags=None, client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, model_name='gpt-3.5-turbo', temperature=0.0, model_kwargs={}, openai_api_key='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', openai_api_base='', openai_organization='', openai_proxy='', request_timeout=None, max_retries=6, streaming=False, n=1, max_tokens=None)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template.messages[0].prompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gbu5qjBXhCnJ",
        "outputId": "621bf4ce-7171-4229-d0e9-2465f6a3dbfd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PromptTemplate(input_variables=['header', 'setting', 'style'], output_parser=None, partial_variables={}, template=' {header}\\nThe setting of the story is: {setting}. The style of the story is {style}. ', template_format='f-string', validate_template=True)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template.messages[0].prompt.input_variables"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q1uxG3tthO00",
        "outputId": "629df9cf-52dd-4eab-c172-1e8b168bafeb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['header', 'setting', 'style']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "user_setting = \"\"\" A story about \\\n",
        "a crazy psychopath female singer set \\\n",
        "in Brooklyn\n",
        "\"\"\"\n",
        "\n",
        "user_style = \"\"\"British English \\\n",
        "in the style of Charles Dickens\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "MTiAYfl4hSuB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_story = prompt_template.format_messages(\n",
        "                    header=header,\n",
        "                    style=user_style,\n",
        "                    setting=user_setting)"
      ],
      "metadata": {
        "id": "PUZrSUqQhbYh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(user_story))\n",
        "print(type(user_story[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fHMpNb-5iNPT",
        "outputId": "1e3eb98b-8ae3-4d10-b0cb-8df8e8f2b6c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n",
            "<class 'langchain.schema.HumanMessage'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(user_story[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FnXzd3TciTSo",
        "outputId": "55d3c256-3276-48e5-c9c9-a8124504ceaf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content=' Act as an interactive storyteller. You act as the narrator and present me with options of how to continue the story. Present me a random number of choices from 2 to 5 every time you produce an output. \\\\ \\nThe setting of the story is:  A story about a crazy psychopath female singer set in Brooklyn\\n. The style of the story is British English in the style of Charles Dickens\\n. ' additional_kwargs={} example=False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Call the LLM to translate to the style of the customer message\n",
        "story = chat(user_story)"
      ],
      "metadata": {
        "id": "26LhDzONiY4F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(story.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EL_6Ef8xij60",
        "outputId": "ebba72f4-9774-42e3-c5c3-48e73ddcadf9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It was a dark and stormy night in Brooklyn, and the streets were deserted except for one lone figure. It was a woman, dressed in a tattered black dress, with wild hair and a crazed look in her eyes. She was singing to herself, a haunting melody that echoed through the empty streets.\n",
            "\n",
            "As she walked, she came across a group of young men, huddled together in a doorway. They looked up as she approached, and she smiled at them, revealing a set of sharp teeth.\n",
            "\n",
            "Option 1: The men run away in fear.\n",
            "Option 2: The woman attacks the men.\n",
            "Option 3: The woman starts singing to the men.\n",
            "Option 4: The woman asks the men for directions.\n",
            "Option 5: The woman ignores the men and continues on her way.\n",
            "\n",
            "Choose your own adventure!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chain for IF-ELSE statements with ChatGPT\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qL6ngG2kqmHc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import ChatPromptTemplate"
      ],
      "metadata": {
        "id": "WADMxl_euen_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_template = \"\"\"Act as an interactive storyteller. \\\n",
        "You act as the narrator and present me with options of \\\n",
        "how to continue the story. Present me a random number \\\n",
        "of choices from 2 to 5 every time you produce an output. \\\n",
        "The setting of the story is: {input}\n",
        "\n",
        "Examples:\n",
        "\n",
        "Example A:\n",
        "\n",
        "Once upon a time, in the bustling streets of Brooklyn, there lived a crazy psychopath female singer named Delilah. She was known for her hauntingly beautiful voice and her erratic behavior. One day, as she was walking down the street, she saw a group of people gathered around a street performer. \\\n",
        "\\\n",
        "What does Delilah do next? \\\n",
        "Option 1. She joins in and starts singing with the street performer. \\\n",
        "Option 2. She becomes jealous of the street performer's attention and attacks him. \\\n",
        "Option 3. She ignores the street performer and continues on her way. \\\n",
        "Option 4. She watches the street performer from a distance, intrigued by his talent. \\\n",
        "Option 5. She approaches the street performer and offers to collaborate with him.\n",
        "\n",
        "Example B:\n",
        "\n",
        "Once upon a time, in the bustling streets of Brooklyn, there lived a young \\\n",
        "woman named Emily. She was unsettled and restless, always searching for something \\\n",
        "more in life. Emily had grown up in a poor family and had to work hard to make ends meet. \\\n",
        "Despite her struggles, she had a fierce determination to succeed. \\\n",
        "One day, as Emily was walking down the street, she saw a flyer for a job opening at \\\n",
        "a local bookstore. She had always loved books and the idea of working in a \\\n",
        "bookstore excited her. She decided to apply for the job. \\ \\\n",
        "\n",
        "Option 1: Emily gets the job and starts working at the bookstore. She meets a kind and wise old man who teaches her the value of books and helps her find her place in the world. \\\n",
        "Option 2: Emily's application is rejected, and she feels discouraged. She decides to take a walk in the park to clear her head. \\\n",
        "Option 3: Emily gets the job, but she quickly realizes that the bookstore is struggling to stay afloat. She decides to take matters into her own hands and comes up with a plan to save the store. \\\n",
        "Option 4: Emily gets the job, but she soon discovers that her boss is a cruel and abusive man. She must decide whether to stay and endure the abuse or quit and risk being unemployed. \\\n",
        "Option 5: Emily gets the job, but she finds it boring and unfulfilling. She starts to wonder if there is more to life than just working in a bookstore. \\\n",
        "\\\n",
        "What will Emily do? Choose your own adventure and see where the story takes you!\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "continue_template = \"\"\"Act as an interactive storyteller. \\\n",
        "Continue the story based on the option selected: {input} \\\n",
        " making sure it makes sense \\\n",
        "chronologically. You may introduce new characters. \\\n",
        "You act as the narrator. Present me a random number \\\n",
        "of choices from 2 to 5 every time you produce an output. Make sure\n",
        "to not repeat options you've given before. \\ \\\n",
        "\n",
        "Examples:\n",
        "\n",
        "Example A:\n",
        "\n",
        "Emily took Sarah's advice and decided to launch a social media campaign to promote her loyalty program. \\\n",
        "She spent hours brainstorming ideas and creating content that would appeal to her target audience. \\\n",
        "Finally, she was ready to launch her campaign. \\\n",
        "\n",
        "What will Emily decide to do next - choose your own adventure! \\ \\\n",
        "\n",
        "Option 1: Emily's campaign goes viral, and she gains thousands of new customers. \\\n",
        "Option 2: Emily's campaign falls flat, and she doesn't see any increase in customers. \\\n",
        "Option 3: Emily's campaign attracts negative attention, and she receives backlash from the public. \\\n",
        "Option 4: Emily's campaign is successful, but she struggles to keep up with the influx of new customers. \\\n",
        "Option 5: Emily's campaign is successful, but she realizes that her loyalty program is not sustainable in the long run.\n",
        "\n",
        "Example B:\n",
        "\n",
        "Marco seeks the advice of a local wise man who is known for his knowledge of the island's history and culture. \\\n",
        "The wise man invites Marco into his home, and they sit down to talk.  \\ \\\n",
        "\n",
        "What does the wise man tell Marco?  \\ \\\n",
        "\n",
        "Option 1: The wise man tells Marco about the island's ancient legends and myths. \\\n",
        "Option 2: The wise man shares stories of the island's past rulers and their accomplishments. \\\n",
        "Option 3: The wise man advises Marco on how to navigate the island's complex social hierarchy. \\\n",
        "Option 4: The wise man warns Marco about dangerous creatures that inhabit the island. \\\n",
        "Option 5: The wise man teaches Marco about the island's unique flora and fauna.\n",
        "\"\"\"\n",
        "\n",
        "end_template = \"\"\"End the story based on the option selected: {input}\n",
        " making sure it makes sense \\\n",
        "chronologically. You may introduce new characters. Just like before, \\\n",
        "act as an interactive storyteller. \\\n",
        "You act as the narrator. Present me a random number \\\n",
        "of choices from 2 to 5 every time you produce an output. Make sure to not\n",
        "repeat options you've given before. \"\"\"\n",
        "\n",
        "\n",
        "user_setting = \"\"\" A story about \\\n",
        "a crazy psychopath female singer set \\\n",
        "in Brooklyn\n",
        "\"\"\"\n",
        "\n",
        "input = \"\"\" A story about \\\n",
        "an unsettled young woman  \\\n",
        "in Brooklyn in British English \\\n",
        "in the style of Charles Dickens\"\"\"\n",
        "\n",
        "prompt_infos = [\n",
        "                {\"name\": \"start story\",\n",
        "                 \"description\": \"Good for starting a story.\",\n",
        "                 \"prompt_template\": start_template\n",
        "                 },\n",
        "                {\"name\": \"continue story\",\n",
        "                 \"description\": \"Good for continuing a story when a user provides an Option.\",\n",
        "                 \"prompt_template\": continue_template},\n",
        "                 {\"name\": \"end story\",\n",
        "                 \"description\": \"Good for continuing a story when a user provides an Option and asks to end the story.\",\n",
        "                 \"prompt_template\": end_template}]"
      ],
      "metadata": {
        "id": "1Z7qmtS217vS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "# start_template = ChatPromptTemplate.from_template(template_string)\n",
        "# continue_template = ChatPromptTemplate.from_template(template_string)\n",
        "\n",
        "# start_story = start_template.format_messages(\n",
        "#                     header=start_header,\n",
        "#                     style=user_style,\n",
        "#                     setting=user_setting)\n",
        "\n",
        "# continue_story = continue_template.format_messages(\n",
        "#                     header=continue_header,\n",
        "#                     style=user_style,\n",
        "#                     setting=user_setting)\n",
        "\n"
      ],
      "metadata": {
        "id": "lh-kQxaxqrAf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.router import MultiPromptChain\n",
        "from langchain.chains.router.llm_router import LLMRouterChain, RouterOutputParser\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.memory import ConversationBufferWindowMemory, ConversationBufferMemory\n",
        "from langchain.chains.router.multi_prompt_prompt import MULTI_PROMPT_ROUTER_TEMPLATE\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "\n",
        "llm = ChatOpenAI(openai_api_key=\"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\", temperature=0.0)"
      ],
      "metadata": {
        "id": "TOlT2mSVs0Q0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "destination_chains = {}\n",
        "for p_info in prompt_infos:\n",
        "    name = p_info[\"name\"]\n",
        "    prompt_template = p_info[\"prompt_template\"]\n",
        "    prompt= ChatPromptTemplate.from_template(template=prompt_template)\n",
        "    chain = LLMChain(llm=llm, prompt=prompt)\n",
        "    destination_chains[name] = chain\n",
        "\n",
        "destinations = [f\"{p['name']}: {p['description']}\" for p in prompt_infos]\n",
        "destinations_str = \"\\n\".join(destinations)\n",
        "\n",
        "default_prompt = ChatPromptTemplate.from_template(continue_template)\n",
        "default_chain = LLMChain(llm=llm, prompt=default_prompt)"
      ],
      "metadata": {
        "id": "SRRW5sI8uAV-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# You may also revise the original input if you think that revising\\\n",
        "# it will ultimately lead to a better response from the language model."
      ],
      "metadata": {
        "id": "dLo9ox61N_jV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MULTI_PROMPT_ROUTER_TEMPLATE = \"\"\"Given a raw text input to a \\\n",
        "# language model select the model prompt best suited for the input. \\\n",
        "# You will be given the names of the available prompts and a \\\n",
        "# description of what the prompt is best suited for. \\\n",
        "\n",
        "\n",
        "# << FORMATTING >>\n",
        "# Return a markdown code snippet with a JSON object formatted to look like:\n",
        "# ```json\n",
        "# {{{{\n",
        "#     \"destination\": string \\ name of the prompt to use or \"DEFAULT\"\n",
        "#     \"next_inputs\": string \\ a potentially modified version of the original input\n",
        "# }}}}\n",
        "# ```\n",
        "\n",
        "# REMEMBER: \"destination\" MUST be one of the candidate prompt \\\n",
        "# names specified below OR it can be \"DEFAULT\" if the input is not\\\n",
        "# well suited for any of the candidate prompts.\n",
        "\n",
        "# << CANDIDATE PROMPTS >>\n",
        "# {destinations}\n",
        "\n",
        "# << INPUT >> {{input}}\n",
        "\n",
        "\n",
        "# << OUTPUT (remember to include the ```json)>>\"\"\""
      ],
      "metadata": {
        "id": "bb-PEfVHuHxz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(\n",
        "    destinations=destinations_str\n",
        ")\n",
        "router_prompt = PromptTemplate(\n",
        "    template=router_template,\n",
        "    input_variables=[\"input\"],\n",
        "    output_parser=RouterOutputParser(),\n",
        ")\n",
        "\n",
        "router_chain = LLMRouterChain.from_llm(llm, router_prompt)"
      ],
      "metadata": {
        "id": "ELjxdBXK1Qdz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "memory = ConversationBufferMemory()\n",
        "chain = MultiPromptChain(router_chain=router_chain,\n",
        "                         destination_chains=destination_chains,\n",
        "                         default_chain=default_chain, verbose=True\n",
        "                        )"
      ],
      "metadata": {
        "id": "co0plnZJuXuJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain.run(\"A story about a Venetian general during the occupation of Cyprus in around 1500.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "id": "xbUZjH-W2xsu",
        "outputId": "549883f8-0417-46bb-e1f9-7610f4ec7530"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new  chain...\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:275: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start story: {'input': 'A story about a Venetian general during the occupation of Cyprus in around 1500.'}\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Once upon a time, in the occupied island of Cyprus, there lived a Venetian general named Marco. He was a skilled warrior and a respected leader, but he was also haunted by the memories of the battles he had fought and the lives he had taken. One day, as Marco was patrolling the streets of Nicosia, he saw a group of Cypriot rebels gathering in a nearby alley. What does Marco do next? Option 1: He orders his soldiers to attack the rebels and crush the rebellion. Option 2: He approaches the rebels and tries to negotiate a peaceful solution. Option 3: He ignores the rebels and continues on his patrol. Option 4: He watches the rebels from a distance, trying to understand their motives and their goals. Option 5: He decides to join the rebels and fight against his own army, hoping to bring an end to the occupation. What will Marco do? Choose your own adventure and see where the story takes you!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain.run(\"Option 4. Marco seeks the advice of a local wise man who is known for his knowledge of the island's history and culture.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "ycS6FhGU2ynC",
        "outputId": "9667a825-e0cb-402c-afdc-6fbd1ad98c89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new  chain...\u001b[0m\n",
            "continue story: {'input': \"Marco seeks the advice of a local wise man who is known for his knowledge of the island's history and culture.\"}\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Marco seeks the advice of a local wise man who is known for his knowledge of the island's history and culture. The wise man invites Marco into his home, and they sit down to talk. \\n\\nWhat does the wise man tell Marco? \\n\\nOption 1: The wise man tells Marco about the island's ancient legends and myths. \\nOption 2: The wise man shares stories of the island's past rulers and their accomplishments. \\nOption 3: The wise man advises Marco on how to navigate the island's complex social hierarchy. \\nOption 4: The wise man warns Marco about dangerous creatures that inhabit the island. \\nOption 5: The wise man teaches Marco about the island's unique flora and fauna.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain.run(\"Option 2: The wise man shares stories of the island's past rulers and their accomplishments.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        },
        "id": "bc8lBMzrq0jg",
        "outputId": "f974c26d-90e8-414e-b7fd-7f72490f4b8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new  chain...\u001b[0m\n",
            "continue story: {'input': \"Option 2: The wise man shares stories of the island's past rulers and their accomplishments.\"}\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The wise man cleared his throat and began to share stories of the island\\'s past rulers. He spoke of King Arin, who had built the grand castle on the hill, and Queen Liria, who had established the island\\'s first school. He spoke of Prince Kael, who had led the island\\'s army to victory in a great war, and Princess Elara, who had brought prosperity to the island through her trade agreements with neighboring kingdoms. \\n\\nAs the wise man spoke, the group listened intently, fascinated by the island\\'s rich history. He shared stories of triumph and tragedy, of great accomplishments and devastating failures. \\n\\nSuddenly, a young girl in the group spoke up. \"But what about Queen Isadora?\" she asked. \"I\\'ve heard that she was the greatest ruler of them all.\" \\n\\nThe wise man smiled. \"Ah, Queen Isadora,\" he said. \"She was indeed a remarkable leader. She was known for her wisdom and her compassion, and she brought peace to the island during a time of great turmoil.\" \\n\\nThe group leaned in, eager to hear more about Queen Isadora\\'s accomplishments. \\n\\nOption 1: The wise man tells the story of how Queen Isadora brokered a peace treaty between two warring factions on the island. \\nOption 2: The wise man shares the tale of how Queen Isadora implemented a groundbreaking education system that allowed all children on the island to receive an education. \\nOption 3: The wise man recounts the time when Queen Isadora led a successful expedition to discover new lands beyond the island\\'s shores. \\nOption 4: The wise man tells the story of how Queen Isadora worked tirelessly to improve the island\\'s infrastructure, building roads and bridges that connected all corners of the island. \\nOption 5: The wise man shares the tale of how Queen Isadora established a system of fair and just laws that ensured equality for all citizens of the island.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Frontend"
      ],
      "metadata": {
        "id": "NIS4A_GK9ZK1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing Gradio"
      ],
      "metadata": {
        "id": "5wWXLcL79a2F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install gradio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SuBwjVYw9fA7",
        "outputId": "2b68de63-bf77-4748-80a7-0d275aeaef02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gradio in /usr/local/lib/python3.10/dist-packages (3.35.2)\n",
            "Requirement already satisfied: aiofiles in /usr/local/lib/python3.10/dist-packages (from gradio) (23.1.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from gradio) (3.8.4)\n",
            "Requirement already satisfied: altair>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.2.2)\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.10/dist-packages (from gradio) (0.97.0)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.10/dist-packages (from gradio) (0.3.0)\n",
            "Requirement already satisfied: gradio-client>=0.2.7 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.2.7)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from gradio) (0.24.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.15.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.2)\n",
            "Requirement already satisfied: markdown-it-py[linkify]>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.2.0)\n",
            "Requirement already satisfied: markupsafe in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: mdit-py-plugins<=0.3.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.3.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from gradio) (1.22.4)\n",
            "Requirement already satisfied: orjson in /usr/local/lib/python3.10/dist-packages (from gradio) (3.9.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from gradio) (1.5.3)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from gradio) (8.4.0)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from gradio) (1.10.7)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: pygments>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.14.0)\n",
            "Requirement already satisfied: python-multipart in /usr/local/lib/python3.10/dist-packages (from gradio) (0.0.6)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from gradio) (2.27.1)\n",
            "Requirement already satisfied: semantic-version in /usr/local/lib/python3.10/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.22.0)\n",
            "Requirement already satisfied: websockets>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (11.0.3)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair>=4.2.0->gradio) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair>=4.2.0->gradio) (4.3.3)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair>=4.2.0->gradio) (0.12.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client>=0.2.7->gradio) (2023.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio-client>=0.2.7->gradio) (23.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from gradio-client>=0.2.7->gradio) (4.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.14.0->gradio) (3.12.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.14.0->gradio) (4.65.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py[linkify]>=2.0.0->gradio) (0.1.2)\n",
            "Requirement already satisfied: linkify-it-py<3,>=1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py[linkify]>=2.0.0->gradio) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->gradio) (2022.7.1)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.14.0->gradio) (8.1.3)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.14.0->gradio) (0.14.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->gradio) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->gradio) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->gradio) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->gradio) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->gradio) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->gradio) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->gradio) (1.3.1)\n",
            "Requirement already satisfied: starlette<0.28.0,>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from fastapi->gradio) (0.27.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->gradio) (2022.12.7)\n",
            "Requirement already satisfied: httpcore<0.18.0,>=0.15.0 in /usr/local/lib/python3.10/dist-packages (from httpx->gradio) (0.17.2)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->gradio) (3.4)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->gradio) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gradio) (1.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gradio) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gradio) (4.39.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gradio) (1.4.4)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gradio) (3.0.9)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->gradio) (1.26.15)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from httpcore<0.18.0,>=0.15.0->httpx->gradio) (3.6.2)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair>=4.2.0->gradio) (0.19.3)\n",
            "Requirement already satisfied: uc-micro-py in /usr/local/lib/python3.10/dist-packages (from linkify-it-py<3,>=1->markdown-it-py[linkify]>=2.0.0->gradio) (1.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->gradio) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n"
      ],
      "metadata": {
        "id": "0Epp63rv9aJ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def change_textbox(choice):\n",
        "    # To update the buttons we can use regex to extract options or\n",
        "    # even use chatGPT and then paste them into list of options\n",
        "    options = [\"a\",\"b\",\"c\"]\n",
        "    options2 = [\"d\",\"e\"]\n",
        "    if choice == \"short\":\n",
        "        return gr.Radio.update(choices=options)\n",
        "    elif choice == \"a\":\n",
        "        return gr.Radio.update(choices=options2)\n",
        "    elif choice == \"long\":\n",
        "        return gr.Textbox.update(lines=8, visible=True)\n",
        "    else:\n",
        "        return gr.Textbox.update(visible=False)\n",
        "\n",
        "\n",
        "with gr.Blocks() as block:\n",
        "    radio = gr.Radio(\n",
        "        [\"short\", \"long\", \"none\"], label=\"What kind of essay would you like to write?\"\n",
        "    )\n",
        "    text = gr.Textbox(lines=2, interactive=True)\n",
        "\n",
        "    radio.change(fn=change_textbox, inputs=radio, outputs=radio)\n",
        "    block.launch(share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 610
        },
        "id": "w97Jrftv9duu",
        "outputId": "e406f122-2c4f-4fa3-8d8f-2b6630c7bda7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://e17b185b35f25b50ed.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://e17b185b35f25b50ed.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import random\n",
        "import time\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    chatbot = gr.Chatbot()\n",
        "    msg = gr.Textbox() # msg = select\n",
        "    clear = gr.ClearButton([msg, chatbot])\n",
        "\n",
        "    def respond(message, chat_history):\n",
        "        bot_message = chain.run(message)\n",
        "        chat_history.append((message, bot_message))\n",
        "        time.sleep(2)\n",
        "        return \"\", chat_history\n",
        "\n",
        "# For second time round (to continue the story) we need to change the first\n",
        "# argument to be the text of the option selected by the user\n",
        "    msg.submit(respond, [msg, chatbot], [msg, chatbot])\n",
        "\n",
        "demo.launch(share=True)\n",
        "\n",
        "# issue with memory also present (check LangChain)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 610
        },
        "id": "xywYahmR-Vt7",
        "outputId": "f5464c4e-8512-4e92-cdd2-b3318738bdc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://edc7618aaca46fd780.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://edc7618aaca46fd780.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "\n",
        "def add_text(history, text):\n",
        "    history = history + [(text, None)]\n",
        "    return history, gr.update(value=\"\", interactive=False)\n",
        "\n",
        "\n",
        "def add_file(history, file):\n",
        "    history = history + [((file.name,), None)]\n",
        "    return history\n",
        "\n",
        "\n",
        "def bot(history):\n",
        "    response = \"**That's cool!**\"\n",
        "    history[-1][1] = response\n",
        "    return history\n",
        "\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    chatbot = gr.Chatbot([], elem_id=\"chatbot\").style(height=750)\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=0.85):\n",
        "            txt = gr.Textbox(\n",
        "                show_label=False,\n",
        "                placeholder=\"Enter text and press enter, or upload an image\",\n",
        "                visible = True\n",
        "            ).style(container=False)\n",
        "        with gr.Column(scale=0.15, min_width=0):\n",
        "            btn = gr.UploadButton(\"📁\", file_types=[\"image\", \"video\", \"audio\"])\n",
        "\n",
        "    txt_msg = txt.submit(add_text, [chatbot, txt], [chatbot, txt], queue=False).then(\n",
        "        bot, chatbot, chatbot\n",
        "    )\n",
        "    txt_msg.then(lambda: gr.update(interactive=True), None, [txt], queue=False)\n",
        "    file_msg = btn.upload(add_file, [chatbot, btn], [chatbot], queue=False).then(\n",
        "        bot, chatbot, chatbot\n",
        "    )\n",
        "\n",
        "demo.launch(share=True)\n"
      ],
      "metadata": {
        "id": "BckeTnLaIVCu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "outputId": "425dd649-85ba-4d5c-b386-71590e22b23e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gradio/components/chatbot.py:228: UserWarning: The `style` method is deprecated. Please set these arguments in the constructor instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gradio/components/textbox.py:259: UserWarning: The `style` method is deprecated. Please set these arguments in the constructor instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://d80ec9aa19f44c8690.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://d80ec9aa19f44c8690.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Regex to split text into a list of the Options"
      ],
      "metadata": {
        "id": "LOBT4rgESSnR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"At the age of 22, Eli had become the youngest and most powerful CEO in the world. Everywhere he went, Eli was seen as a symbol of power and success. One day, while visiting the seaside hamlet of Mallenton, he was suddenly declared the new modern day Caesar. What does Eli do next? Option 1. He embraces his newfound power and sets about transforming the town according to his own vision. Option 2. He rejects his title and shuns the attention, wanting only to be known as Eli. Option 3. He decides to use his power to unite the people and bring peace to the town. Option 4. He creates a new set of laws to govern the town by decree. Option 5. He builds a palace to celebrate his newfound status.\"\n",
        "\n",
        "# Remove everything after \"Option 1.\"\n",
        "text = text.split(\"Option 1.\")[0]\n",
        "\n",
        "print(text)\n"
      ],
      "metadata": {
        "id": "eyKsQTDrJEkw",
        "outputId": "d664d8e3-1265-4b3e-c378-3cd062ca7356",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "At the age of 22, Eli had become the youngest and most powerful CEO in the world. Everywhere he went, Eli was seen as a symbol of power and success. One day, while visiting the seaside hamlet of Mallenton, he was suddenly declared the new modern day Caesar. What does Eli do next? \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "text = \"Marco seeks the advice of a local wise man who is known for his knowledge of the island's history and culture. The wise man invites Marco into his home, and they sit down to talk.\\n\\nWhat does the wise man tell Marco?\\n\\nOption 1: The wise man tells Marco about the island's ancient legends and myths.\\nOption 2: The wise man shares stories of the island's past rulers and their accomplishments.\\nOption 3: The wise man advises Marco on how to navigate the island's complex social hierarchy.\\nOption 4: The wise man warns Marco about dangerous creatures that inhabit the island.\\nOption 5: The wise man teaches Marco about the island's unique flora and fauna.\"\n",
        "\n",
        "options = re.findall(r\"Option \\d+: .+\", text)\n",
        "options = [option.strip() for option in options]\n",
        "\n",
        "print(options)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FUpC8VYBM2N-",
        "outputId": "df18f779-177e-43d7-99d8-ed2ca683f51b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"Option 1: The wise man tells Marco about the island's ancient legends and myths.\", \"Option 2: The wise man shares stories of the island's past rulers and their accomplishments.\", \"Option 3: The wise man advises Marco on how to navigate the island's complex social hierarchy.\", 'Option 4: The wise man warns Marco about dangerous creatures that inhabit the island.', \"Option 5: The wise man teaches Marco about the island's unique flora and fauna.\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def split_options(output):\n",
        "    options = re.split(r\"Option \\d+\\.\", output)[1:]\n",
        "    options = [option.strip() for option in options]\n",
        "\n",
        "    # options = re.findall(r\"Option \\d+: .+\", output)\n",
        "    # options = [option.strip() for option in options]\n",
        "\n",
        "    return options\n",
        "\n",
        "output = \"Once upon a time, there lived a modern day Napoleon. He went by the name of Charles and he had a dream of conquering all of Europe. To achieve this goal, he chose to start with the Netherlands, a small but powerful country in Central Europe. How does Charles begin his conquest? Option 1. He sends assassins to eliminate important government officials. Option 2. He builds an army and marches onto Dutch soil. Option 3. He rallies like-minded followers and stirs up political unrest from within. Option 4. He launches a surprise attack using the latest military technology. Option 5. He forms economic and political alliances with wealthy Dutch patrons.\"\n",
        "\n",
        "print(split_options(output))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_N4yO2ijzc_r",
        "outputId": "3ac38496-7b21-4447-cfc1-77fc97b0d141"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "DCfqUG6ezb5i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "options"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AkYp_h0JR_EO",
        "outputId": "4f6dc442-dd95-4e6d-8fb9-6abfbd87b3ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"Option 1: The wise man tells Marco about the island's ancient legends and myths.\",\n",
              " \"Option 2: The wise man shares stories of the island's past rulers and their accomplishments.\",\n",
              " \"Option 3: The wise man advises Marco on how to navigate the island's complex social hierarchy.\",\n",
              " 'Option 4: The wise man warns Marco about dangerous creatures that inhabit the island.',\n",
              " \"Option 5: The wise man teaches Marco about the island's unique flora and fauna.\"]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "soBScRz1SAbN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Rewrite Backend with memory and ConversationChain"
      ],
      "metadata": {
        "id": "UGsXxglqpP6m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.memory import ConversationBufferWindowMemory, ConversationBufferMemory\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chains import ConversationChain, LLMChain"
      ],
      "metadata": {
        "id": "_AKH_Am_pTzj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_template = \"\"\"Act as an interactive storyteller. \\\n",
        "You act as the narrator and present me with options of \\\n",
        "how to continue the story. Present me a random number \\\n",
        "of choices from 2 to 5 every time you produce an output. \\\n",
        "After beginning the story, you will be provided an option \\\n",
        "on how to continue the story. When this is done you should \\\n",
        "continue the story and then provide a random number of choices \\\n",
        "from 2 to 5 again. Continue this and do not end the story unless \\\n",
        "you are specifically told to. \\\n",
        "\\\n",
        "The setting of the story: {input} \\\n",
        "\\\n",
        "\n",
        "Here are some examples of inputs and outputs for both starting the \\\n",
        "story and continuining the story \\\n",
        "\n",
        "Start Example: \\\n",
        "\n",
        "Input: Write me a story about a crazy psychopath singer in Brooklyn \\ \\\n",
        "\n",
        "Output:\n",
        "\n",
        "Once upon a time, in the bustling streets of Brooklyn, there lived a crazy psychopath female singer named Delilah. She was known for her hauntingly beautiful voice and her erratic behavior. One day, as she was walking down the street, she saw a group of people gathered around a street performer. \\\n",
        "\\\n",
        "What does Delilah do next? \\\n",
        "Option 1. She joins in and starts singing with the street performer. \\\n",
        "Option 2. She becomes jealous of the street performer's attention and attacks him. \\\n",
        "Option 3. She ignores the street performer and continues on her way. \\\n",
        "Option 4. She watches the street performer from a distance, intrigued by his talent. \\\n",
        "Option 5. She approaches the street performer and offers to collaborate with him. \\\n",
        "\\\n",
        "\n",
        "Continue Example: \\\n",
        "\n",
        "Input:  Option 3: Marco gets the advice of a local wise man. \\\n",
        "\\\n",
        "Output:\n",
        "Marco seeks the advice of a local wise man who is known for his knowledge of the island's history and culture. \\\n",
        "The wise man invites Marco into his home, and they sit down to talk.  \\ \\\n",
        "\n",
        "What does the wise man tell Marco?  \\ \\\n",
        "\n",
        "Option 1: The wise man tells Marco about the island's ancient legends and myths. \\\n",
        "Option 2: The wise man shares stories of the island's past rulers and their accomplishments. \\\n",
        "Option 3: The wise man advises Marco on how to navigate the island's complex social hierarchy. \\\n",
        "Option 4: The wise man warns Marco about dangerous creatures that inhabit the island. \\\n",
        "Option 5: The wise man teaches Marco about the island's unique flora and fauna.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "JFjGV8KGpqy4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain.run(\"Write me a story about a Venetian general in Cyprus.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "WD5wGC_8qfhh",
        "outputId": "6940f5bf-d2f1-45de-993c-70825929c3ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "''"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import FewShotPromptTemplate\n",
        "\n",
        "# create our examples\n",
        "EXAMPLES = [\n",
        "    {\n",
        "        \"input\": \" Write me a story about a crazy psychopath singer in Brooklyn \",\n",
        "        \"answer\": \"Once upon a time, in the bustling streets of Brooklyn, there lived a crazy psychopath female singer named Mary. She was known for her hauntingly beautiful voice and her erratic behavior. One day, as she was walking down the street she saw a group of people gathered around a street performer. What does she do next? Option 1. She joins in and starts singing with the street performer. Option 2. She becomes jealous of the street performer's attention and attacks him. Option 3. She ignores the street performer and continues on her way. Option 4. She watches the street performer from a distance, intrigued by his talent. Option 5. She approaches the street performer and offers to collaborate with him. \"\n",
        "    }, { \"input\": \"Option 3: Marco gets the advice of a local wise man.\",\n",
        "        \"answer\": \"Marco seeks the advice of a local wise man who is known for his knowledge of the island's history and culture. The wise man invites him into his home and they sit down to talk. What does the wise man tell him? Option 1: The wise man tells Marco about the island's ancient legends and myths. Option 2: The wise man shares stories of the island's past rulers and their accomplishments. Option 3: The wise man advises him on how to navigate the island's complex social hierarchy.\"},\n",
        "        {\"input\":\"Write a story about an Egyptian Pharaoh in hiding from his enemies.\",\n",
        "        \"answer\":\" Once upon a time, in the depths of ancient Egypt, lived a powerful Pharaoh named Akhenaten.  He had many enemies, eager to seize his throne and overtake the kingdom. In order to stay safe and protect his people, Akhenaten had to hide from his enemies and find somewhere safe to stay. Where does he go? Option 1. He seeks refuge in a nearby temple. Option 2. He heads to the desert and builds a secret underground sanctuary. Option 3. He takes shelter in a forgotten tomb in the heart of the Valley of the Kings.  Option 4. He goes to a fortress in the east, on the banks of the Red Sea.  Option 5. He disguises himself as a peasant and lives amongst the common folk.\"\n",
        "    }]\n",
        "\n",
        "\n",
        "\n",
        "# examples = [{\n",
        "#     \"query\": \"Write me a story about a Venetian general in Cyprus\",\n",
        "#     \"answer\":\n",
        "# \"\"\"\n",
        "# Once upon a time, in the Mediterranean island of Cyprus, there lived a Venetian general named Marco. He was tasked with leading a small army of soldiers to protect the island from invaders. As Marco and his troops marched through the island, they encountered a small village. What does Marco do next? Option 1. He orders his troops to attack the village. Option 2. He orders his troops to search the village for any suspicious activity. Option 3. He orders his troops to provide aid to the villagers. Option 4. He orders his troops to set up camp in the village. Option 5. He orders his troops to ignore the village and continue on their mission.\n",
        "# Are follow up questions needed here: Yes.\n",
        "# Follow up: Option 1. He orders his troops to attack the village.\n",
        "# Intermediate answer:  Marco orders his troops to search the village for any suspicious activity. As they search, they come across a strange looking man who is wearing a hooded cloak. The man is acting suspiciously and refuses to answer any of the soldiers' questions. What does Marco do next? Option 1. He orders his troops to arrest the man and bring him to the castle. Option 2. He orders his troops to ignore the man and continue searching the village. Option 3. He orders his troops to interrogate the man and find out what he is up to. Option 4. He orders his troops to escort the man out of the village. Option 5. He orders his troops to search the man for any weapons or contraband.\n",
        "# Follow up: Option 1. He orders his troops to arrest the man and bring him to the castle.\n",
        "# Intermediate answer:  Marco orders his troops to arrest the man and bring him to the castle. When they arrive, Marco interrogates the man to find out why he was in the forest. What does the man tell Marco? Option 1. The man tells Marco that he was searching for a magical artifact. Option 2. The man tells Marco that he was looking for a hidden treasure. Option 3. The man tells Marco that he was trying to escape from a rival kingdom. Option 4. The man tells Marco that he was trying to find a lost family member. Option 5. The man tells Marco that he was trying to find a cure for a mysterious illness.\n",
        "# Follow up: Option 3. The man tells Marco that he was trying to escape from a rival kingdom and END the story.\n",
        "# So the final answer is: The man tells Marco he was trying to escape from the Ottoman empire. Marco forgives the man and gives him a the Doge's pardon, he then sets of again to continue monitoring his kingdom. The End.\n",
        "# \"\"\"\n",
        "#   }]\n",
        "\n",
        "\n",
        "\n",
        "# create a example template\n",
        "EXAMPLE_TEMPLATE = \"\"\"\n",
        "User: {input}\n",
        "AI: {answer}\n",
        "\"\"\"\n",
        "\n",
        "# create a prompt example from above template\n",
        "example_prompt = PromptTemplate(\n",
        "    input_variables=[\"input\", \"answer\"],\n",
        "    template=EXAMPLE_TEMPLATE\n",
        ")\n",
        "\n",
        "# now break our previous prompt into a prefix and suffix\n",
        "# the prefix is our instructions\n",
        "PREFIX = \"\"\"Act as an interactive storyteller. \\\n",
        "You act as the narrator and present me with options of \\\n",
        "how to continue the story. Present me a random number \\\n",
        "of Options between 2 to 5 every time you produce an output. \\\n",
        "After beginning the story, you will be provided an option \\\n",
        "on how to continue the story. When this is done you should \\\n",
        "continue the story and then provide a random number of Options \\\n",
        "between 2 to 5 again. Continue this and do not end the story unless \\\n",
        "you are specifically told to. Make sure you give your characters names\\\n",
        "The following are examples of inputs \\\n",
        "and outputs. The conversation history is {history}\n",
        "\"\"\"\n",
        "\n",
        "#  If you start with a character named Alex, \\\n",
        "# then make sure you keep that name for the rest of the conversation.\n",
        "# and the suffix our user input and output indicator\n",
        "SUFFIX = \"\"\"\n",
        "User: {input}\n",
        "AI: \"\"\"\n",
        "\n",
        "# now create the few shot prompt template\n",
        "few_shot_prompt_template = FewShotPromptTemplate(\n",
        "    examples=EXAMPLES,\n",
        "    example_prompt=example_prompt,\n",
        "    prefix=PREFIX,\n",
        "    suffix=SUFFIX,\n",
        "    input_variables=[\"input\", \"history\"],\n",
        "    example_separator=\"\\n\\n\"\n",
        ")"
      ],
      "metadata": {
        "id": "KGShApxEr80N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"Act as an interactive storyteller. \\\n",
        "You act as the narrator and present me with options of \\\n",
        "how to continue the story. Present me a random number \\\n",
        "of choices from 2 to 5 every time you produce an output. \\\n",
        "After beginning the story, you will be provided an option \\\n",
        "on how to continue the story. When this is done you should \\\n",
        "continue the story and then provide a random number of options \\\n",
        "from 2 to 5 again. \\\n",
        "Continue this and do not end the story unless \\\n",
        "you are specifically told to. {input}\n",
        "\"\"\"\n",
        "user_story = PromptTemplate(template=template, input_variables=[\"input\"])"
      ],
      "metadata": {
        "id": "z2aUrjt0UMzo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = OpenAI(temperature=1,openai_api_key=\"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\")\n",
        "memory = ConversationBufferMemory()\n",
        "\n",
        "# PROMPT = PromptTemplate(input_variables=[\"query\"], template=few_shot_prompt_template)\n",
        "chain = ConversationChain(\n",
        "    llm=llm,\n",
        "    verbose=False, memory = memory, prompt=few_shot_prompt_template\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "WIX2Ezef50yL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain.run(\"Πες μου μια αφηγηση για ενα παιδι που εχει προβληματα στο σχολειο με τον βατραχο του\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "1h7sU0oA9ND0",
        "outputId": "872ae02b-21d9-400b-c304-d22a83a3f0a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nOnce upon a time, there was a young boy named Paul who had a pet frog that he kept in his pocket at all times. Paul was always getting into trouble at school because his frog tended to jump out of his pocket during class, causing a disruption. No matter how hard Paul tried to keep his frog contained, it always seemed to escape. How does he solve this problem? Option 1: He lets the teacher know about his frog and apologizes for any disruption it has caused. Option 2: He creates a small bag for his frog to keep it secure during school. Option 3: He takes his frog to school in a jar with holes so it can breathe. Option 4: He leaves his frog in a safe place before entering the school building. Option 5: He trains his frog to stay in his pocket and behave during school.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "memory"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UIgA0oVdPDE0",
        "outputId": "d4f688be-cee8-46d7-bf4e-86020c9c6916"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ConversationBufferMemory(chat_memory=ChatMessageHistory(messages=[HumanMessage(content='Write me a story about a Greek rapper called Jay Aspros who starts his career in Athens', additional_kwargs={}, example=False), AIMessage(content='\\nOnce upon a time, in the vibrant city of Athens, there lived a soon-to-be famous rapper called Jay Aspros. Aspiring to become a successful artist, Jay cultivated his talent and studied the newest hip-hop trends. But before he could launch his career, he needed to make a name for himself in the music scene. How does he go about doing this? Option 1: He releases a mix of homemade songs online. Option 2: He parks outside of clubs and raps to anyone who passes by. Option 3: He starts a rap battle campaign at local open mics. Option 4: He organizes live performances to show off his skills. Option 5: He collaborates with popular DJs in the city.', additional_kwargs={}, example=False), HumanMessage(content='Option 3: He starts a rap battle campaign at local open mics', additional_kwargs={}, example=False), AIMessage(content='\\nJay embarks on a rap battle campaign by entering local open mics. Word begins to spread and soon, Jay is leaving the crowd in awe with his lyrical prowess. But with so much hype, more and more people are beginning to show up at his performances. What does he do in order to maintain his growing fanbase? Option 1: He offers to perform for free at different venues in exchange for exposure. Option 2: He begins to increase the length of his performances to keep the energy up. Option 3: He begins to collaborate with other rappers in the city. Option 4: He takes up a side-hustle as a music producer. Option 5: He takes his rap battle show on the road and begins to tour other cities.', additional_kwargs={}, example=False), HumanMessage(content='Option 5: He takes his rap battle show on the road and begins to tour other cities.', additional_kwargs={}, example=False), AIMessage(content=' Jay takes his rap battle show on the road and begins to tour other cities. Word spreads about his show and soon, large crowds start gathering to watch him in action. As he continues on his tour, he finds himself competing against the best rappers the country has to offer. How does he prove his worth in the face of such competition?  Option 1: He delivers passionate and powerful performances that leave the audience in awe. Option 2: He incorporates elements of other genres such as rock, jazz, and soul into his sets. Option 3: He challenges his opponents to rap battles, drawing large crowds. Option 4: He leverages his connections in the music industry to draw a bigger audience. Option 5: He offers free mixtapes of his show to supplement his tour.', additional_kwargs={}, example=False), HumanMessage(content='Πες μου μια αφηγηση για ενα παιδι που εχει προβληματα στο σχολειο με τον βατραχο του', additional_kwargs={}, example=False), AIMessage(content='\\nOnce upon a time, there was a young boy named Paul who had a pet frog that he kept in his pocket at all times. Paul was always getting into trouble at school because his frog tended to jump out of his pocket during class, causing a disruption. No matter how hard Paul tried to keep his frog contained, it always seemed to escape. How does he solve this problem? Option 1: He lets the teacher know about his frog and apologizes for any disruption it has caused. Option 2: He creates a small bag for his frog to keep it secure during school. Option 3: He takes his frog to school in a jar with holes so it can breathe. Option 4: He leaves his frog in a safe place before entering the school building. Option 5: He trains his frog to stay in his pocket and behave during school.', additional_kwargs={}, example=False)]), output_key=None, input_key=None, return_messages=False, human_prefix='Human', ai_prefix='AI', memory_key='history')"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import FewShotPromptTemplate\n",
        "\n",
        "# create our examples\n",
        "EXAMPLES = [\n",
        "    {\n",
        "        \"input\": \" Write me a story about a crazy psychopath singer in Brooklyn \",\n",
        "        \"answer\": \"Once upon a time, in the bustling streets of Brooklyn, there lived a crazy psychopath female singer named Mary. She was known for her hauntingly beautiful voice and her erratic behavior. One day, as she was walking down the street she saw a group of people gathered around a street performer. What does she do next? Option 1. She joins in and starts singing with the street performer. Option 2. She becomes jealous of the street performer's attention and attacks him. Option 3. She ignores the street performer and continues on her way. Option 4. She watches the street performer from a distance, intrigued by his talent. Option 5. She approaches the street performer and offers to collaborate with him. \"\n",
        "    }, { \"input\": \"Option 3: Marco gets the advice of a local wise man.\",\n",
        "        \"answer\": \"Marco seeks the advice of a local wise man who is known for his knowledge of the island's history and culture. The wise man invites him into his home and they sit down to talk. What does the wise man tell him? Option 1: The wise man tells Marco about the island's ancient legends and myths. Option 2: The wise man shares stories of the island's past rulers and their accomplishments. Option 3: The wise man advises him on how to navigate the island's complex social hierarchy.\"},\n",
        "        {\"input\":\"Write a story about an Egyptian Pharaoh in hiding from his enemies.\",\n",
        "        \"answer\":\" Once upon a time, in the depths of ancient Egypt, lived a powerful Pharaoh named Akhenaten.  He had many enemies, eager to seize his throne and overtake the kingdom. In order to stay safe and protect his people, Akhenaten had to hide from his enemies and find somewhere safe to stay. Where does he go? Option 1. He seeks refuge in a nearby temple. Option 2. He heads to the desert and builds a secret underground sanctuary. Option 3. He takes shelter in a forgotten tomb in the heart of the Valley of the Kings.  Option 4. He goes to a fortress in the east, on the banks of the Red Sea.  Option 5. He disguises himself as a peasant and lives amongst the common folk.\"\n",
        "    }]\n",
        "\n",
        "\n",
        "\n",
        "# examples = [{\n",
        "#     \"query\": \"Write me a story about a Venetian general in Cyprus\",\n",
        "#     \"answer\":\n",
        "# \"\"\"\n",
        "# Once upon a time, in the Mediterranean island of Cyprus, there lived a Venetian general named Marco. He was tasked with leading a small army of soldiers to protect the island from invaders. As Marco and his troops marched through the island, they encountered a small village. What does Marco do next? Option 1. He orders his troops to attack the village. Option 2. He orders his troops to search the village for any suspicious activity. Option 3. He orders his troops to provide aid to the villagers. Option 4. He orders his troops to set up camp in the village. Option 5. He orders his troops to ignore the village and continue on their mission.\n",
        "# Are follow up questions needed here: Yes.\n",
        "# Follow up: Option 1. He orders his troops to attack the village.\n",
        "# Intermediate answer:  Marco orders his troops to search the village for any suspicious activity. As they search, they come across a strange looking man who is wearing a hooded cloak. The man is acting suspiciously and refuses to answer any of the soldiers' questions. What does Marco do next? Option 1. He orders his troops to arrest the man and bring him to the castle. Option 2. He orders his troops to ignore the man and continue searching the village. Option 3. He orders his troops to interrogate the man and find out what he is up to. Option 4. He orders his troops to escort the man out of the village. Option 5. He orders his troops to search the man for any weapons or contraband.\n",
        "# Follow up: Option 1. He orders his troops to arrest the man and bring him to the castle.\n",
        "# Intermediate answer:  Marco orders his troops to arrest the man and bring him to the castle. When they arrive, Marco interrogates the man to find out why he was in the forest. What does the man tell Marco? Option 1. The man tells Marco that he was searching for a magical artifact. Option 2. The man tells Marco that he was looking for a hidden treasure. Option 3. The man tells Marco that he was trying to escape from a rival kingdom. Option 4. The man tells Marco that he was trying to find a lost family member. Option 5. The man tells Marco that he was trying to find a cure for a mysterious illness.\n",
        "# Follow up: Option 3. The man tells Marco that he was trying to escape from a rival kingdom and END the story.\n",
        "# So the final answer is: The man tells Marco he was trying to escape from the Ottoman empire. Marco forgives the man and gives him a the Doge's pardon, he then sets of again to continue monitoring his kingdom. The End.\n",
        "# \"\"\"\n",
        "#   }]\n",
        "\n",
        "\n",
        "\n",
        "# create a example template\n",
        "EXAMPLE_TEMPLATE = \"\"\"\n",
        "User: {input}\n",
        "AI: {answer}\n",
        "\"\"\"\n",
        "\n",
        "# now break our previous prompt into a prefix and suffix\n",
        "# the prefix is our instructions\n",
        "PREFIX = \"\"\"Act as an interactive storyteller. \\\n",
        "You act as the narrator and present me with options of \\\n",
        "how to continue the story. Present me a random number \\\n",
        "of Options between 2 to 5 every time you produce an output. \\\n",
        "After beginning the story, you will be provided an option \\\n",
        "on how to continue the story. When this is done you should \\\n",
        "continue the story and then provide a random number of Options \\\n",
        "between 2 to 5 again. Continue this and do not end the story unless \\\n",
        "you are specifically told to. Make sure you give your characters names\\\n",
        "The following are examples of inputs \\\n",
        "and outputs. The conversation history is {history}\n",
        "\"\"\"\n",
        "\n",
        "#  If you start with a character named Alex, \\\n",
        "# then make sure you keep that name for the rest of the conversation.\n",
        "# and the suffix our user input and output indicator\n",
        "SUFFIX = \"\"\"\n",
        "User: {input}\n",
        "AI: \"\"\"\n",
        "\n",
        "def launch_chain():\n",
        "  example_prompt = PromptTemplate(\n",
        "    input_variables=[\"input\", \"answer\"],\n",
        "    template=EXAMPLE_TEMPLATE\n",
        ")\n",
        "  few_shot_prompt_template = FewShotPromptTemplate(\n",
        "    examples=EXAMPLES,\n",
        "    example_prompt=example_prompt,\n",
        "    prefix=PREFIX,\n",
        "    suffix=SUFFIX,\n",
        "    input_variables=[\"input\", \"history\"],\n",
        "    example_separator=\"\\n\\n\"\n",
        ")\n",
        "  llm = OpenAI(temperature=1,openai_api_key=\"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\")\n",
        "  memory = ConversationBufferMemory()\n",
        "\n",
        "# PROMPT = PromptTemplate(input_variables=[\"query\"], template=few_shot_prompt_template)\n",
        "  chain = ConversationChain(\n",
        "    llm=llm,\n",
        "    verbose=False, memory = memory, prompt=few_shot_prompt_template\n",
        ")\n",
        "  return chain"
      ],
      "metadata": {
        "id": "X-edmbcjPoV_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = launch_chain()"
      ],
      "metadata": {
        "id": "OYWcR5sjyCnO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain.run(\"Πες μου μια αφηγηση για ενα παιδι που εχει προβληματα στο σχολειο με τον βατραχο του\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "s1OOY9CIzEn7",
        "outputId": "fd1b705f-1b33-42cb-8d93-df91f53c05be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Once upon a time, there lived a young boy named Johnny who had a problem at school with his bully, the Rhino. No matter how hard he tried, the Rhino always found a way to make Johnny’s day at school miserable. Johnny had reached the end of his rope and had to find a way to get rid of the Rhino once and for all. What does Johnny decide to do? Option 1. He decides to take matters into his own hands and confront the Rhino. Option 2. He makes a plan to embarrass the Rhino in front of everyone in school. Option 3. He hides from the Rhino and hopes he eventually forgets about him. Option 4. He turns to his parents for help. Option 5. He strikes a deal with the Rhino in private.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain.run(\"Option 2. He makes a plan to embarrass the Rhino in front of everyone in school\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "fflx8xthzJLe",
        "outputId": "93422367-71b5-4108-e27d-e33aa1ddd817"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Johnny comes up with a plan to humiliate the Rhino in front of the entire school. He sets up a big prank and hides to watch how it unfolds. The next day, the prank goes as planned and the entire school is laughing and pointing at the Rhino. How does the Rhino react? Option 1. He gets angry and starts looking for who set up the prank. Option 2. He ignores the laughter and goes about his day as usual. Option 3. He starts to cry in front of the whole school. Option 4. He retaliates by setting up a prank of his own. Option 5. He acknowledges his embarrassment and apologizes to the students he bullied.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.memory import ConversationBufferWindowMemory, ConversationBufferMemory\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chains import ConversationChain, LLMChain\n",
        "from langchain import FewShotPromptTemplate\n",
        "\n",
        "class Storyteller:\n",
        "    def __init__(self):\n",
        "        self.chain = self.launch_chain()\n",
        "\n",
        "    def launch_chain(self):\n",
        "        EXAMPLES = [\n",
        "            {\n",
        "                \"input\": \" Write me a story about a crazy psychopath singer in Brooklyn \",\n",
        "                \"answer\": \"Once upon a time, in the bustling streets of Brooklyn, there lived a crazy psychopath female singer named Mary. She was known for her hauntingly beautiful voice and her erratic behavior. One day, as she was walking down the street she saw a group of people gathered around a street performer. What does she do next? Option 1. She joins in and starts singing with the street performer. Option 2. She becomes jealous of the street performer's attention and attacks him. Option 3. She ignores the street performer and continues on her way. Option 4. She watches the street performer from a distance, intrigued by his talent. Option 5. She approaches the street performer and offers to collaborate with him. \"\n",
        "            },\n",
        "            {\n",
        "                \"input\": \"Option 3: Marco gets the advice of a local wise man.\",\n",
        "                \"answer\": \"Marco seeks the advice of a local wise man who is known for his knowledge of the island's history and culture. The wise man invites him into his home and they sit down to talk. What does the wise man tell him? Option 1: The wise man tells Marco about the island's ancient legends and myths. Option 2: The wise man shares stories of the island's past rulers and their accomplishments. Option 3: The wise man advises him on how to navigate the island's complex social hierarchy.\"\n",
        "            },\n",
        "            {\n",
        "                \"input\": \"Write a story about an Egyptian Pharaoh in hiding from his enemies.\",\n",
        "                \"answer\": \" Once upon a time, in the depths of ancient Egypt, lived a powerful Pharaoh named Akhenaten.  He had many enemies, eager to seize his throne and overtake the kingdom. In order to stay safe and protect his people, Akhenaten had to hide from his enemies and find somewhere safe to stay. Where does he go? Option 1. He seeks refuge in a nearby temple. Option 2. He heads to the desert and builds a secret underground sanctuary. Option 3. He takes shelter in a forgotten tomb in the heart of the Valley of the Kings.  Option 4. He goes to a fortress in the east, on the banks of the Red Sea.  Option 5. He disguises himself as a peasant and lives amongst the common folk.\"\n",
        "            }\n",
        "        ]\n",
        "\n",
        "        EXAMPLE_TEMPLATE = \"\"\"\n",
        "        User: {input}\n",
        "        AI: {answer}\n",
        "        \"\"\"\n",
        "\n",
        "        PREFIX = \"\"\"Act as an interactive storyteller. \\\n",
        "        You act as the narrator and present me with options of \\\n",
        "        how to continue the story. Present me a random number \\\n",
        "        of Options between 2 to 5 every time you produce an output. \\\n",
        "        After beginning the story, you will be provided an option \\\n",
        "        on how to continue the story. When this is done you should \\\n",
        "        continue the story and then provide a random number of Options \\\n",
        "        between 2 to 5 again. Continue this and do not end the story unless \\\n",
        "        you are specifically told to. Make sure you give your characters names\\\n",
        "        The following are examples of inputs \\\n",
        "        and outputs. The conversation history is {history}\n",
        "        \"\"\"\n",
        "\n",
        "        SUFFIX = \"\"\"\n",
        "        User: {input}\n",
        "        AI: \"\"\"\n",
        "\n",
        "        example_prompt = PromptTemplate(\n",
        "            input_variables=[\"input\", \"answer\"],\n",
        "            template=EXAMPLE_TEMPLATE\n",
        "        )\n",
        "        few_shot_prompt_template = FewShotPromptTemplate(\n",
        "            examples=EXAMPLES,\n",
        "            example_prompt=example_prompt,\n",
        "            prefix=PREFIX,\n",
        "            suffix=SUFFIX,\n",
        "            input_variables=[\"input\", \"history\"],\n",
        "            example_separator=\"\\n\\n\"\n",
        "        )\n",
        "\n",
        "        llm = OpenAI(temperature=1, openai_api_key=\"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\")\n",
        "        memory = ConversationBufferMemory()\n",
        "\n",
        "        chain = ConversationChain(\n",
        "            llm=llm,\n",
        "            verbose=False,\n",
        "            memory=memory,\n",
        "            prompt=few_shot_prompt_template\n",
        "        )\n",
        "        return chain\n",
        "\n",
        "    def run_chain(self, input):\n",
        "        return self.chain.run(input)\n",
        "\n",
        "\n",
        "# Usage:\n",
        "storyteller = Storyteller()\n",
        "output = storyteller.run_chain(\"Write me a story about a Venetian general in Cyprus\")\n",
        "print(output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EvlLNb-VzQG-",
        "outputId": "46382ec7-0721-4a46-84f9-d8f1f5129fa1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Once upon a time, a Venetian general named Antonio was tasked with leading a fleet of soldiers to gain control of the island of Cyprus. He sailed to the island with his troops and upon arriving they faced a difficult decision. What do they do next? Option 1. Antonio decides to march into the city and try to take control by force. Option 2. Antonio realizes the island is heavily fortified and opts for a more diplomatic approach. Option 3. Antonio leads his troops to the shores and they begin to set up camp. Option 4. Antonio scopes out the city and plans a sneak attack. Option 5. Antonio sends his men to search for allies and spies among the locals.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vipcmwnl8wuN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}